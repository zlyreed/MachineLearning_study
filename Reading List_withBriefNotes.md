-[Five Questions Machine Learning can answer](https://brohrer.github.io/five_questions_data_science_answers.html)
1. Is this A or B?
2. Is this weird?
3. How much/how many?
4. How is it organized?
5. What should I do next?

-[No, Machine Learning is not just glorified Statistics](https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3?source=emailShare-17a8687279ce-1547271903&_branch_match_id=613367988687238058)
 - Machine Learning Does Not Require An Advanced Knowledge of Statistics
 - Machine Learning = Representation + Evaluation + Optimization: Once you have the evaluation component, you can optimize the representation function in order to improve your evaluation metric. In neural networks, this usually means using some variant of stochastic gradient descent to update the weights and biases of your network according to some defined loss function. 
 - Regression Over 100 Million Variables—No Problem: The VGG-16 ConvNet architecture, for example, has approximately 138 million parameters. 
 
 [A Tour of The Top 10 Algorithms for Machine Learning Newbies](https://medium.com/cracking-the-data-science-interview/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-7228aa8ef541): In a nutshell, it states that no one algorithm works best for every problem, and it’s especially relevant for supervised learning (i.e. predictive modeling).
 - The Big Principle: Machine learning algorithms are described as learning a target function (f) that best maps input variables (X) to an output variable (Y): Y = f(X)
 - 1—Linear Regression
 - 2—Logistic Regression: It is the go-to method for binary classification problems (problems with two class values).
 - 3—Linear Discriminant Analysis: If you have more than two classes then the Linear Discriminant Analysis algorithm is the preferred linear classification technique.
 - 4—Classification and Regression Trees: Trees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.
 - 5—Naive Bayes: The model is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. 
 - 6—K-Nearest Neighbors
 - 7—Learning Vector Quantization
 - 8—Support Vector Machines (SVM)
 - 9—Bagging and Random Forest
 - 10—Boosting and AdaBoost
 
 